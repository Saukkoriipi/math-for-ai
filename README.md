# Introduction to the Mathematics of Deep Learning

This repository contains **teaching materials** for an **introductory lecture on the mathematics of deep learning**.  
It includes **Python code, Jupyter notebooks, and lecture slides (PowerPoint)** to illustrate key concepts.

âš ï¸ **Note:** The **lecture slides are in Finnish**, but all **code, comments, and notebooks are in English**.

---

## Contents

* ğŸ“‚ **Code & Notebooks**
  * Implementations of **linear** and **cubic regression** trained with **stochastic gradient descent (SGD)**.
  * Step-by-step visualization of the training process (loss curves, fitted models).
  * Examples of **forward passes, normalization, gradients, and updates** written in simple Python/Numpy.

* ğŸ“‚ **Lecture Slides**
  * PowerPoint presentations introducing the **mathematical foundations**.
  * Covers: normalization, gradient descent, loss functions, chain rule, and regression as a gateway to neural networks.

---

## Repository Structure

```

lecture-materials/
â”‚
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ height\_prediction\_demo.ipynb
â”‚
â”œâ”€â”€ slides/
â”‚   â”œâ”€â”€ lecture.pdf
â”‚   â””â”€â”€ lecture.pptx
â”‚
â””â”€â”€ README.md

```

---

## Models in Code

* **Linear Model**

$$
\hat{y} = w x + b
$$

* **Cubic Model**

$$
\hat{y} = w_1 x + w_2 x^2 + w_3 x^3 + b
$$

Both trained with **vanilla gradient descent**.

---

## Mathematics

### Normalization

Inputs and outputs are standardized with **z-score normalization**:

$$
x_{norm} = \frac{x - \mu}{\sigma}, \qquad y_{norm} = \frac{y - \mu}{\sigma}
$$

### Loss Function

Training minimizes the **Mean Squared Error (MSE)**:

$$
\mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2
$$

### Gradients via Chain Rule

For the cubic model:

$$
\hat{y} = w_1 x + w_2 x^2 + w_3 x^3 + b, \quad e = \hat{y} - y
$$

$$
\frac{\partial \ell}{\partial w_1} = 2 e x, \quad
\frac{\partial \ell}{\partial w_2} = 2 e x^2, \quad
\frac{\partial \ell}{\partial w_3} = 2 e x^3, \quad
\frac{\partial \ell}{\partial b} = 2 e
$$

---

## Learning Objectives

* Understand **how gradient descent works** at the mathematical level.  
* Learn to derive gradients step by step using the **chain rule**.  
* Recognize the basic components of gradient-based **deep learning optimization**.  
* Observe in real time how training updates the model parameters and how this shapes the **loss landscape** and training dynamics.  

---

## How to Use

1. Open the Jupyter notebooks to run the code interactively.
2. Watch the training process step by step (with visual plots).
3. Follow along with the **PowerPoint slides (Finnish)** for mathematical explanations.
4. Compare the performance of **linear vs cubic models**.

---

## License

Free to use for **educational purposes**. Attribution is appreciated when reusing in your own teaching.

---

## Author & University

**Author:** Mikko Saukkoriipi  
**University:** Aalto University  
**Date:** 2025-09-14